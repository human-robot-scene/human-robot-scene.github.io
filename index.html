<!DOCTYPE html>
<html lang="en">
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta charset="utf-8">
  <title>Human-Robot-Scene Interaction and Collaboration</title>
  <link rel="stylesheet" href="style.css">
</head>

<body>
  <div class="nav">
    <div class="nav-container">
      <img src="assets/iccv-navbar-logo.svg" class="nav-logo" alt="ICCV Logo">
      <a href="">Home</a>
      <a href="#intro">Introduction</a>
      <a href="#call">Call for Papers</a>
      <a href="#challenge">Challenge</a>
      <a href="#speakers">Speakers</a>
    </div>
  </div>

  <div class="title-container">
    <div class="overlay"></div>
    <div class="content" style="text-align: center; margin: 20px">
      <h1>Human-Robot-Scene Interaction and Collaboration</h1>
      <div class="subtitle">
        <a href="https://iccv.thecvf.com/">ICCV 2025</a> Workshop
      </div>
      <div class="subtitle">Oct 20th (Afternoon), 2025</div>
      <div class="subtitle">Honolulu, Hawai'i</div>
    </div>
  </div>

  <div class="container">
    <div class="section" id="intro">
      <h2>Introduction</h2>

      <p>
        Intelligent robots are advancing rapidly, with embodied agents increasingly expected to work and live alongside humans in households, factories, hospitals, schools, <i>etc</i>. For these agents to operate safely, socially, and intelligently, <b>they must effectively interact with humans and adapt to changing environments</b>. Moreover, <b>such interactions can transform human behavior and even reshape the environment</b>‚Äîfor example, through adjustments in human motion during robot-assisted handovers or the redesign of objects for improved robotic grasping. Beyond established research in human-human and human-scene interactions, vast opportunities remain in exploring human-robot-scene collaboration. This workshop will explore the integration of embodied agents into dynamic human-robot-scene interactions. Our focus is on, but not limited to:

        <ul>
          <li>Transferring knowledge from human-human and human-scene interaction and collaboration to inform the development of humanoids and other embodied agents (e.g., via retargeting).</li>
          <li>Exploring different methods for deriving visual representations that capture object properties, dynamics, and affordances relevant to human-robot collaboration.</li>
          <li>Investigating methods for modeling and predicting human intentions to enable robots to anticipate actions and respond safely.</li>
          <li>Integrating robots into interactive settings to foster seamless and effective teamwork.</li>
          <li>Establishing meaningful benchmarks and metrics to measure advancements in human-robot-scene interaction and collaboration.</li>
        </ul>
      </p>
    </div>

    <div class="section" id="call">
      <h2>Call for Papers</h2>
      <p>
        We invite submissions of <strong>long papers</strong> (up to 8 pages excluding references) and <strong>short papers</strong> (up to 4 pages excluding references) that explore human-robot-scene interaction and collaboration.
      </p>

      <h3>Paper Topics</h3>
      <p>
        Topics of interests include, but are not limited to:
        <ul>
          <li><strong>Visual Representations</strong>: Developing methods to derive visual representations that capture object properties, dynamics, and affordances pertinent to human-robot collaboration.</li>
          <li><strong>Scene Understanding</strong>: Enhancing the ability to understand and interpret dynamic environments, including the recognition of objects, actions, and interactions.</li>
          <li><strong>Motion Capture and Human Interaction Modeling</strong>: Advancing techniques for capturing and modeling human motion and interactions.</li>
          <li><strong>Knowledge Transfer</strong>: Leveraging insights from human-human and human-scene interactions to inform the development of humanoid and other embodied agents, such as through motion retargeting techniques.</li>
          <li><strong>Intention Modeling</strong>: Investigating approaches for modeling and predicting human intentions to enable robots to anticipate actions and respond safely.</li>
          <li><strong>Human-Robot Collaboration</strong>: Exploring strategies for integrating robots into interactive settings to foster seamless and effective teamwork.</li>
          <li><strong>Benchmarking and Metrics</strong>: Establishing meaningful benchmarks and metrics to assess advancements in human-robot-scene interaction and collaboration.</li>
        </ul>
      </p>
      
      <p>
        We invite submissions that address the topic of <strong>human-robot-scene interaction and collaboration</strong>. If you have any doubts about whether your submission aligns with the workshop's focus, please feel free to reach out to <a href="mailto:gujy1@shanghaitech.edu.cn">us</a> for clarification.
      </p>

      <h3>Submission Guidelines</h3>
      <ul>
        <li><strong>Paper Format</strong>: Submissions should adhere to the <a href="https://iccv.thecvf.com/Conferences/2025/AuthorGuidelines">ICCV 2025 formatting guidelines</a>. Authors may upload optional supplementary materials, containing additional details, videos, images, etc.</li>
        <li><strong>Length</strong>: Papers must not exceed 8 pages, excluding references.</li>
        <li><strong>Review Process</strong>: All submissions will undergo a double-blind peer-review process.</li>
        <li><strong>Submission Portal</strong>: <a href="https://openreview.net/group?id=thecvf.com/ICCV/2025/Workshop/HRSIC" target="_blank">Submit Your Paper</a></li>
        <li><strong>Non-Archived</strong>: Accepted papers will <strong>not</strong> be published in the official ICCV workshop proceedings. For authors who want to submit their accepted work at this workshop to a different journal or conference, please check their double submissions policy.</li>
        <li>Accepted papers will be presented in a poster session and selected papers as spotlight talks.</li>
      </ul>
      
      <h3>Important Dates</h3>
      <ul>
        <li><strong>Paper Submission Deadline</strong>: August 8th, 2025</li>
        <li><strong>Notification of Acceptance</strong>: August 22th, 2025</li>
        <li><strong>Camera-Ready Submission</strong>: August 29th, 2025</li>
        <li><strong>Workshop Date</strong>: October 20, 2025</li>
      </ul>
    </div>

    <div class="section" id="challenge">
      <h2>Challenge</h2>
      <!-- <img src="assets/terrain-challenge.png" alt="Terrain Challenge Logo"> -->
      <p>
        We are excited to announce the <strong>Multi-Terrain Humanoid Locomotion Challenge</strong>, which will be held in conjunction with the workshop. The challenge aims to foster advancements in humanoid-scene interaction by providing a platform for researchers to showcase their work on embodied agents in dynamic environments. For more details, please visit the <a href="https://human-robot-scene.github.io/Terrain-Challenge/">challenge website</a>.
      </p>
      <!-- <p>
        <b style="color: red">üèÜ Awards</b>: ü•á First Prize ($1000) ü•à Second Prize ($500) ü•â Third Prize ($300)
      </p> -->
    </div>
    <!-- https://dexterous-grasping.github.io/ -->

    <div class="section" id="speakers">
      <h2>Invited Speakers</h2>
      <p> listed alphabetically </p>
      <div class="people">
        <a href="https://shuangli59.github.io/">
          <img src="assets/shuangli-photo.jpeg">
          <div>Shuang Li</div>
          <div class="affiliation">Stanford University</div>
        </a>
        <a href="https://roozbehm.info/">
          <img src="assets/profile_roozbehM.jpg">
          <div>Roozbeh Mottaghi</div>
          <div class="affiliation">Meta AI</div>
        </a>
        <a href="https://www.iit.it/people-details/-/people/lorenzo-natale">
          <img src="assets/lorenzo_natale.jpeg">
          <div>Lorenzo Natale</div>
          <div class="affiliation">Italian Institute of Technology</div>
        </a>
        <a href="https://www.lerrelpinto.com/">
          <img src="assets/lerrel_pinto.jpg">
          <div>Lerrel Pinto</div>
          <div class="affiliation">New York University</div>
        </a>
        <a href="https://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html">
          <img src="assets/gpm.png">
          <div>Gerard Pons-Moll</div>
          <div class="affiliation">University of T√ºbingen</div>
        </a>
      </div>
    </div>

    <div class="section" id="organizers">
      <h2>Organizers</h2>
      <!-- <p> listed alphabetically </p> -->
      <div class="people">
        <a href="https://jiayuan-gu.github.io/">
          <img src="assets/jiayuan_gu.jpg">
          <div>Jiayuan Gu</div>
          <div class="affiliation">ShanghaiTech University</div>
        </a>
        <a href="https://xingyu-lin.github.io/">
          <img src="assets/xinyu_lin.png">
          <div>Xingyu Lin</div>
          <div class="affiliation">OpenAI</div>
        </a>
        <a href="https://fangchenliu.github.io/">
          <img src="assets/fangchen.jpg">
          <div>Fangchen Liu</div>
          <div class="affiliation">UC Berkeley</div>
        </a>
        <a href="https://yuexinma.me/aboutme.html">
          <img src="assets/yuexin_ma.jpg">
          <div>Yuexin Ma</div>
          <div class="affiliation">ShanghaiTech University</div>
        </a>
        <a href="https://www.oru.se/english/employee/martin_magnusson">
          <img src="assets/martin_magnusson.jpg">
          <div>Martin Magnusson</div>
          <div class="affiliation">√ñrebro University</div>
        </a>
        <a href="https://robotics.shanghaitech.edu.cn/people/soeren">
          <img src="assets/Soeren_1cropped.jpg">
          <div>S√∂ren Schwertfeger</div>
          <div class="affiliation">ShanghaiTech University</div>
        </a>
        <a href="https://shiye21.github.io/">
          <img src="assets/yeshi.jpg">
          <div>Ye Shi</div>
          <div class="affiliation">ShanghaiTech University</div>
        </a>
        <a href="https://cseweb.ucsd.edu/~haosu/">
          <img src="assets/haosu.jpeg">
          <div>Hao Su</div>
          <div class="affiliation">UC San Diego</div>
        </a>
        <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/">
          <img src="assets/jingya.jpg">
          <div>Jingya Wang</div>
          <div class="affiliation">ShanghaiTech University</div>
        </a>
        <a href="http://www.xu-lan.com/">
          <img src="assets/lan.jpg">
          <div>Lan Xu</div>
          <div class="affiliation">ShanghaiTech University</div>
        </a>
        <a href="https://ericyi.github.io/">
          <img src="assets/ericyi.jpg">
          <div>Li Yi</div>
          <div class="affiliation">Tsinghua University</div>
        </a>
      </div>
    </div>

    <div class="section" id="sponsors">
      <h2>Sponsors</h2>
    </div>
  </div>
  
  <footer>
    <p>&copy; 2025 ICCV Workshop on Human-Robot-Scene Interaction and Collaboration</p>
  </footer>
</body>

</html>